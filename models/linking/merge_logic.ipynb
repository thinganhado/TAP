{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2c4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11730/3306476163.py:31: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  day_int = geo_day.view(\"int64\") // 86_400_000_000_000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'low': 1341882, 'mid': 5240513, 'high': 163370}\n",
      "chosen high threshold: 0.41931943492766466\n",
      "Wrote rows 0 to 250000\n",
      "Wrote rows 250000 to 500000\n",
      "Wrote rows 500000 to 750000\n",
      "Wrote rows 750000 to 1000000\n",
      "Wrote rows 1000000 to 1250000\n",
      "Wrote rows 1250000 to 1500000\n",
      "Wrote rows 1500000 to 1750000\n",
      "Wrote rows 1750000 to 2000000\n",
      "Wrote rows 2000000 to 2250000\n",
      "Wrote rows 2250000 to 2500000\n",
      "Wrote rows 2500000 to 2750000\n",
      "Wrote rows 2750000 to 3000000\n",
      "Wrote rows 3000000 to 3250000\n",
      "Wrote rows 3250000 to 3500000\n",
      "Wrote rows 3500000 to 3750000\n",
      "Wrote rows 3750000 to 4000000\n",
      "Wrote rows 4000000 to 4250000\n",
      "Wrote rows 4250000 to 4500000\n",
      "Wrote rows 4500000 to 4750000\n",
      "Wrote rows 4750000 to 5000000\n",
      "Wrote rows 5000000 to 5250000\n",
      "Wrote rows 5250000 to 5500000\n",
      "Wrote rows 5500000 to 5750000\n",
      "Wrote rows 5750000 to 6000000\n",
      "Wrote rows 6000000 to 6250000\n",
      "Wrote rows 6250000 to 6500000\n",
      "Wrote rows 6500000 to 6745765\n",
      "\n",
      "Done. Wrote\n",
      "  /home/opc/datasets/mbd_mini_dataset/geo_linked.csv.gz\n",
      "  /home/opc/datasets/mbd_mini_dataset/tx_linked.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# paths\n",
    "GEO_CSV = \"/home/opc/datasets/mbd_mini_dataset/geo_data_with_features.csv\"\n",
    "TX_CSV  = \"/home/opc/datasets/PS_20174392719_1491204439457_log.csv\"\n",
    "OUT_DIR = \"/home/opc/datasets/mbd_mini_dataset\"\n",
    "GEO_OUT = f\"{OUT_DIR}/geo_linked.csv.gz\"\n",
    "TX_OUT  = f\"{OUT_DIR}/tx_linked.csv.gz\"\n",
    "\n",
    "# knobs\n",
    "LOW_Q = 0.20\n",
    "MID_FRAUD_RATE = 0.50\n",
    "REUSE_FACTOR = 20         # used to set the high cut automatically\n",
    "BATCH_SIZE = 250_000      # tune for memory\n",
    "RANDOM_SEED = 13\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def load_geo_min(path: str) -> pd.DataFrame:\n",
    "    use = [\"client_id\",\"event_time\",\"composite_suspicion_score\",\"latitude\",\"longitude\"]\n",
    "    head = pd.read_csv(path, nrows=0)\n",
    "    use = [c for c in use if c in head.columns]\n",
    "    geo = pd.read_csv(path, usecols=use, low_memory=False)\n",
    "    if \"event_time\" in geo.columns:\n",
    "        try:\n",
    "            geo[\"event_time\"] = pd.to_datetime(geo[\"event_time\"])\n",
    "            geo_day = geo[\"event_time\"].dt.floor(\"D\")\n",
    "            # convert to integer day for fast matching, NaT goes to -1\n",
    "            day_int = geo_day.view(\"int64\") // 86_400_000_000_000\n",
    "            day_int = day_int.fillna(-1).astype(np.int64)\n",
    "            geo[\"geo_day_int\"] = day_int\n",
    "        except Exception:\n",
    "            geo[\"geo_day_int\"] = -1\n",
    "    else:\n",
    "        geo[\"geo_day_int\"] = -1\n",
    "    return geo\n",
    "\n",
    "def load_tx_min(path: str) -> pd.DataFrame:\n",
    "    tx = pd.read_csv(path, low_memory=False)\n",
    "    need = [\"isFraud\",\"nameOrig\",\"step\"]\n",
    "    if not set(need).issubset(tx.columns):\n",
    "        raise ValueError(f\"tx needs columns {need}\")\n",
    "    tx[\"isFraud\"] = pd.to_numeric(tx[\"isFraud\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    tx[\"tx_day_int\"] = (pd.to_numeric(tx[\"step\"], errors=\"coerce\").fillna(0) // 24).astype(np.int64)\n",
    "    return tx\n",
    "\n",
    "def pick_high_cut(css: pd.Series, n_tx_fraud: int, reuse_factor: int) -> float:\n",
    "    target_high = max(n_tx_fraud * reuse_factor, n_tx_fraud)\n",
    "    target_high = min(target_high, len(css))\n",
    "    q = 1.0 - target_high / len(css)\n",
    "    q = float(np.clip(q, 0.90, 0.9999))\n",
    "    return css.quantile(q)\n",
    "\n",
    "def band_and_labels(geo: pd.DataFrame, low_q=LOW_Q, high_thr=None) -> tuple[pd.Series, np.ndarray]:\n",
    "    css = geo[\"composite_suspicion_score\"].astype(float)\n",
    "    low_thr = css.quantile(low_q)\n",
    "    band = np.where(css >= high_thr, \"high\", np.where(css <= low_thr, \"low\", \"mid\"))\n",
    "    # labels, high to 1, low to 0, mid Bernoulli\n",
    "    labels = np.zeros(len(geo), dtype=np.int8)\n",
    "    labels[band == \"high\"] = 1\n",
    "    mid_mask = band == \"mid\"\n",
    "    n_mid = int(mid_mask.sum())\n",
    "    if n_mid:\n",
    "        labels[mid_mask] = rng.binomial(1, MID_FRAUD_RATE, size=n_mid).astype(np.int8)\n",
    "    return pd.Series(band, index=geo.index, name=\"geo_band\"), labels\n",
    "\n",
    "def build_pools(tx: pd.DataFrame):\n",
    "    # return global index arrays and per day dicts for each label\n",
    "    tx_idx = np.arange(len(tx), dtype=np.int64)\n",
    "    fraud_mask = tx[\"isFraud\"].values == 1\n",
    "    non_mask   = ~fraud_mask\n",
    "    pools = {\n",
    "        1: {\n",
    "            \"idx\": tx_idx[fraud_mask],\n",
    "            \"by_day\": {},\n",
    "        },\n",
    "        0: {\n",
    "            \"idx\": tx_idx[non_mask],\n",
    "            \"by_day\": {},\n",
    "        }\n",
    "    }\n",
    "    # group by day for each label\n",
    "    for lbl in [1, 0]:\n",
    "        sub = tx.loc[pools[lbl][\"idx\"], [\"tx_day_int\"]].reset_index(drop=True)\n",
    "        # map back positions into pools[lbl][\"idx\"]\n",
    "        positions = np.arange(len(sub), dtype=np.int64)\n",
    "        by_day = {}\n",
    "        for day, pos_arr in pd.Series(positions).groupby(sub[\"tx_day_int\"]):\n",
    "            by_day[int(day)] = pools[lbl][\"idx\"][pos_arr.values]\n",
    "        pools[lbl][\"by_day\"] = by_day\n",
    "    return pools\n",
    "\n",
    "def choose_indices_for_batch(day_ints: np.ndarray, labels: np.ndarray, pools: dict) -> np.ndarray:\n",
    "    n = len(day_ints)\n",
    "    out = np.empty(n, dtype=np.int64)\n",
    "    # process by label, then by day, vectorized choice\n",
    "    for lbl in [1, 0]:\n",
    "        lbl_mask = labels == lbl\n",
    "        if not lbl_mask.any():\n",
    "            continue\n",
    "        days = day_ints[lbl_mask]\n",
    "        # unique days in this label\n",
    "        uniq_days, counts = np.unique(days, return_counts=True)\n",
    "        # for each day group, choose from day pool if available, else from global pool\n",
    "        for day, cnt in zip(uniq_days, counts):\n",
    "            if day in pools[lbl][\"by_day\"] and len(pools[lbl][\"by_day\"][day]) > 0:\n",
    "                pool = pools[lbl][\"by_day\"][day]\n",
    "            else:\n",
    "                pool = pools[lbl][\"idx\"]\n",
    "            picks = pool[rng.integers(0, len(pool), size=cnt)]\n",
    "            out[lbl_mask & (day_ints == day)] = picks\n",
    "    return out\n",
    "\n",
    "def write_two_csvs_streaming(geo: pd.DataFrame, tx: pd.DataFrame,\n",
    "                             geo_idx: np.ndarray, tx_idx: np.ndarray,\n",
    "                             geo_out: str, tx_out: str, start_link_id: int):\n",
    "    # slice once for speed, then write aligned\n",
    "    geo_slice = geo.iloc[geo_idx].copy()\n",
    "    tx_slice  = tx.iloc[tx_idx].copy()\n",
    "\n",
    "    # add link_id and label\n",
    "    link_ids = np.arange(start_link_id, start_link_id + len(geo_slice), dtype=np.int64)\n",
    "    labels = tx_slice[\"isFraud\"].astype(int).values\n",
    "\n",
    "    geo_slice.insert(0, \"link_id\", link_ids)\n",
    "    geo_slice.insert(1, \"label_isFraud\", labels)\n",
    "\n",
    "    tx_slice.insert(0, \"link_id\", link_ids)\n",
    "    tx_slice.insert(1, \"label_isFraud\", labels)\n",
    "\n",
    "    # on first write, include header, then append\n",
    "    mode = \"x\" if not Path(geo_out).exists() else \"a\"\n",
    "    header = mode == \"x\"\n",
    "    geo_slice.to_csv(geo_out, index=False, compression=\"gzip\", mode=\"w\" if header else \"a\", header=header)\n",
    "    tx_slice.to_csv(tx_out, index=False, compression=\"gzip\", mode=\"w\" if header else \"a\", header=header)\n",
    "\n",
    "def main_fast():\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    geo = load_geo_min(GEO_CSV)\n",
    "    tx  = load_tx_min(TX_CSV)\n",
    "\n",
    "    n_fraud = int((tx[\"isFraud\"] == 1).sum())\n",
    "    high_thr = pick_high_cut(geo[\"composite_suspicion_score\"].astype(float), n_fraud, REUSE_FACTOR)\n",
    "    band, labels = band_and_labels(geo, low_q=LOW_Q, high_thr=high_thr)\n",
    "    geo = geo.assign(geo_band=band.values)\n",
    "\n",
    "    print({\"low\": int((band == \"low\").sum()),\n",
    "           \"mid\": int((band == \"mid\").sum()),\n",
    "           \"high\": int((band == \"high\").sum())})\n",
    "    print(\"chosen high threshold:\", high_thr)\n",
    "\n",
    "    pools = build_pools(tx)\n",
    "\n",
    "    # remove old outputs if present\n",
    "    for p in [GEO_OUT, TX_OUT]:\n",
    "        try:\n",
    "            Path(p).unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    link_id = 0\n",
    "    n = len(geo)\n",
    "    for start in range(0, n, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, n)\n",
    "        batch_idx = np.arange(start, end, dtype=np.int64)\n",
    "\n",
    "        day_ints = geo.loc[batch_idx, \"geo_day_int\"].values\n",
    "        lbls     = labels[batch_idx]\n",
    "\n",
    "        # pick tx indices for this batch, preferring same day pool, else global pool\n",
    "        tx_indices = choose_indices_for_batch(day_ints, lbls, pools)\n",
    "\n",
    "        # write aligned chunks to the two CSVs\n",
    "        write_two_csvs_streaming(\n",
    "            geo=geo[[\"client_id\",\"event_time\",\"latitude\",\"longitude\",\"composite_suspicion_score\",\"geo_day_int\",\"geo_band\"]],\n",
    "            tx=tx[[\"step\",\"type\",\"amount\",\"nameOrig\",\"oldbalanceOrg\",\"newbalanceOrig\",\"nameDest\",\"oldbalanceDest\",\"newbalanceDest\",\"isFraud\",\"isFlaggedFraud\",\"tx_day_int\"]],\n",
    "            geo_idx=batch_idx,\n",
    "            tx_idx=tx_indices,\n",
    "            geo_out=GEO_OUT,\n",
    "            tx_out=TX_OUT,\n",
    "            start_link_id=link_id\n",
    "        )\n",
    "        link_id += len(batch_idx)\n",
    "        print(f\"Wrote rows {start} to {end}\")\n",
    "\n",
    "    print(f\"\\nDone. Wrote\\n  {GEO_OUT}\\n  {TX_OUT}\")\n",
    "\n",
    "main_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5e9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/opc/datasets/mbd_mini_dataset/geo_linked.csv.gz size MB: 121.424672\n",
      "/home/opc/datasets/mbd_mini_dataset/tx_linked.csv.gz size MB: 230.815509\n",
      "\n",
      "GEO columns: ['link_id', 'label_isFraud', 'client_id', 'event_time', 'latitude', 'longitude', 'composite_suspicion_score', 'geo_day_int', 'geo_band']\n",
      "TX  columns: ['link_id', 'label_isFraud', 'step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud', 'tx_day_int']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "GEO_OUT = \"/home/opc/datasets/mbd_mini_dataset/geo_linked.csv.gz\"\n",
    "TX_OUT  = \"/home/opc/datasets/mbd_mini_dataset/tx_linked.csv.gz\"\n",
    "\n",
    "for p in [GEO_OUT, TX_OUT]:\n",
    "    print(Path(p), \"size MB:\", Path(p).stat().st_size / 1e6)\n",
    "\n",
    "# read a small sample for a quick glance\n",
    "geo_head = pd.read_csv(GEO_OUT, nrows=5)\n",
    "tx_head  = pd.read_csv(TX_OUT,  nrows=5)\n",
    "print(\"\\nGEO columns:\", list(geo_head.columns))\n",
    "print(\"TX  columns:\", list(tx_head.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d1937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud ratio, geo file:\n",
      "{'rows': 6745765, 'fraud': 2783163, 'non_fraud': 3962602, 'fraud_rate': 0.4125792997532526}\n",
      "\n",
      "Fraud ratio, tx file:\n",
      "{'rows': 6745765, 'fraud': 2783163, 'non_fraud': 3962602, 'fraud_rate': 0.4125792997532526}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fraud_ratio(path, label_col=\"label_isFraud\", chunksize=500_000):\n",
    "    total = 0\n",
    "    fraud = 0\n",
    "    for chunk in pd.read_csv(path, usecols=[label_col], chunksize=chunksize):\n",
    "        vc = chunk[label_col].value_counts()\n",
    "        total += int(vc.sum())\n",
    "        fraud += int(vc.get(1, 0))\n",
    "    rate = fraud / total if total else 0\n",
    "    return {\"rows\": total, \"fraud\": fraud, \"non_fraud\": total - fraud, \"fraud_rate\": rate}\n",
    "\n",
    "print(\"\\nFraud ratio, geo file:\")\n",
    "print(fraud_ratio(GEO_OUT))\n",
    "\n",
    "print(\"\\nFraud ratio, tx file:\")\n",
    "print(fraud_ratio(TX_OUT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbc8402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "link_id stats, geo: {'rows': 6745765, 'has_duplicates': False}\n",
      "link_id stats, tx:  {'rows': 6745765, 'has_duplicates': False}\n",
      "\n",
      "Row counts, geo 7, tx 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get basic counts and uniqueness for link_id without full load\n",
    "def link_id_stats(path, id_col=\"link_id\", chunksize=500_000):\n",
    "    total = 0\n",
    "    any_dupe = False\n",
    "    seen = set()\n",
    "    for chunk in pd.read_csv(path, usecols=[id_col], chunksize=chunksize):\n",
    "        s = chunk[id_col]\n",
    "        total += len(s)\n",
    "        if s.duplicated().any():\n",
    "            any_dupe = True\n",
    "        # optional, skip storing all ids to save memory\n",
    "    return {\"rows\": total, \"has_duplicates\": any_dupe}\n",
    "\n",
    "print(\"\\nlink_id stats, geo:\", link_id_stats(GEO_OUT))\n",
    "print(\"link_id stats, tx: \", link_id_stats(TX_OUT))\n",
    "\n",
    "# exact equality of row counts between files\n",
    "geo_rows = sum(1 for _ in pd.read_csv(GEO_OUT, usecols=[\"link_id\"], chunksize=1_000_000))\n",
    "tx_rows  = sum(1 for _ in pd.read_csv(TX_OUT,  usecols=[\"link_id\"], chunksize=1_000_000))\n",
    "print(f\"\\nRow counts, geo {geo_rows}, tx {tx_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d2ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unioned 500,000 pairs, nodes 291,332\n",
      "Unioned 1,000,000 pairs, nodes 567,833\n",
      "Unioned 1,500,000 pairs, nodes 831,169\n",
      "Unioned 2,000,000 pairs, nodes 1,081,180\n",
      "Unioned 2,500,000 pairs, nodes 1,316,445\n",
      "Unioned 3,000,000 pairs, nodes 1,548,463\n",
      "Unioned 3,500,000 pairs, nodes 1,766,486\n",
      "Unioned 4,000,000 pairs, nodes 1,972,268\n",
      "Unioned 4,500,000 pairs, nodes 2,172,665\n",
      "Unioned 5,000,000 pairs, nodes 2,362,655\n",
      "Unioned 5,500,000 pairs, nodes 2,543,611\n",
      "Unioned 6,000,000 pairs, nodes 2,716,825\n",
      "Unioned 6,500,000 pairs, nodes 2,882,005\n",
      "Unioned 6,745,765 pairs, nodes 2,960,624\n",
      "Built 28 entities across 2,960,624 nodes\n",
      "Wrote 500,000 rows with entity_id\n",
      "Wrote 1,000,000 rows with entity_id\n",
      "Wrote 1,500,000 rows with entity_id\n",
      "Wrote 2,000,000 rows with entity_id\n",
      "Wrote 2,500,000 rows with entity_id\n",
      "Wrote 3,000,000 rows with entity_id\n",
      "Wrote 3,500,000 rows with entity_id\n",
      "Wrote 4,000,000 rows with entity_id\n",
      "Wrote 4,500,000 rows with entity_id\n",
      "Wrote 5,000,000 rows with entity_id\n",
      "Wrote 5,500,000 rows with entity_id\n",
      "Wrote 6,000,000 rows with entity_id\n",
      "Wrote 6,500,000 rows with entity_id\n",
      "Wrote 6,745,765 rows with entity_id\n",
      "Wrote /home/opc/datasets/mbd_mini_dataset/entity_summary.csv.gz, entities 28\n",
      "\n",
      "Done\n",
      "Geo with entity, /home/opc/datasets/mbd_mini_dataset/geo_linked_with_entity.csv.gz\n",
      "Tx  with entity, /home/opc/datasets/mbd_mini_dataset/tx_linked_with_entity.csv.gz\n",
      "Entity summary, /home/opc/datasets/mbd_mini_dataset/entity_summary.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE = \"/home/opc/datasets/mbd_mini_dataset\"\n",
    "GEO_IN  = f\"{BASE}/geo_linked.csv.gz\"\n",
    "TX_IN   = f\"{BASE}/tx_linked.csv.gz\"\n",
    "GEO_OUT = f\"{BASE}/geo_linked_with_entity.csv.gz\"\n",
    "TX_OUT  = f\"{BASE}/tx_linked_with_entity.csv.gz\"\n",
    "ENT_SUM = f\"{BASE}/entity_summary.csv.gz\"\n",
    "\n",
    "CHUNK = 500_000  # tune for RAM and speed\n",
    "\n",
    "# ---------- Union-Find that can add nodes on the fly ----------\n",
    "class DSU:\n",
    "    __slots__ = (\"parent\", \"rank\")\n",
    "    def __init__(self):\n",
    "        self.parent = np.empty(0, dtype=np.int64)\n",
    "        self.rank   = np.empty(0, dtype=np.int8)\n",
    "    def _add(self, n_new: int):\n",
    "        if n_new <= 0:\n",
    "            return\n",
    "        start = len(self.parent)\n",
    "        self.parent = np.append(self.parent, np.arange(start, start + n_new, dtype=np.int64))\n",
    "        self.rank   = np.append(self.rank,   np.zeros(n_new, dtype=np.int8))\n",
    "    def add_nodes_until(self, max_idx: int):\n",
    "        need = max_idx + 1 - len(self.parent)\n",
    "        if need > 0:\n",
    "            self._add(need)\n",
    "    def find(self, x: int) -> int:\n",
    "        # iterative path compression\n",
    "        p = self.parent\n",
    "        while x != p[x]:\n",
    "            p[x] = p[p[x]]\n",
    "            x = p[x]\n",
    "        return x\n",
    "    def union(self, a: int, b: int):\n",
    "        p = self.parent\n",
    "        r = self.rank\n",
    "        ra = self.find(a)\n",
    "        rb = self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if r[ra] < r[rb]:\n",
    "            p[ra] = rb\n",
    "        elif r[ra] > r[rb]:\n",
    "            p[rb] = ra\n",
    "        else:\n",
    "            p[rb] = ra\n",
    "            r[ra] += 1\n",
    "\n",
    "# ---------- Build entities by streaming the pairs ----------\n",
    "def build_entity_map(geo_path: str, tx_path: str, chunk: int = CHUNK):\n",
    "    dsu = DSU()\n",
    "    node_index = {}  # maps \"g:<id>\" or \"t:<id>\" to int index\n",
    "    next_idx = 0\n",
    "\n",
    "    # iterate both files in lockstep\n",
    "    geo_iter = pd.read_csv(geo_path, usecols=[\"link_id\", \"client_id\"], chunksize=chunk)\n",
    "    tx_iter  = pd.read_csv(tx_path,  usecols=[\"link_id\", \"nameOrig\"],  chunksize=chunk)\n",
    "\n",
    "    total_pairs = 0\n",
    "    for gi, ti in zip(geo_iter, tx_iter):\n",
    "        # alignment check, same link_id order\n",
    "        if not np.array_equal(gi[\"link_id\"].values, ti[\"link_id\"].values):\n",
    "            raise RuntimeError(\"link_id order mismatch between geo and tx chunks, sort files by link_id then rerun\")\n",
    "\n",
    "        g_ids = gi[\"client_id\"].astype(str).values\n",
    "        t_ids = ti[\"nameOrig\"].astype(str).values\n",
    "\n",
    "        # map to integer node ids\n",
    "        g_keys = [\"g:\" + s for s in g_ids]\n",
    "        t_keys = [\"t:\" + s for s in t_ids]\n",
    "\n",
    "        # assign indices for any new keys\n",
    "        for key in np.concatenate([g_keys, t_keys]):\n",
    "            if key not in node_index:\n",
    "                node_index[key] = next_idx\n",
    "                next_idx += 1\n",
    "\n",
    "        # ensure DSU arrays can hold new nodes\n",
    "        dsu.add_nodes_until(next_idx - 1)\n",
    "\n",
    "        # union all pairs in this chunk\n",
    "        for gk, tk in zip(g_keys, t_keys):\n",
    "            dsu.union(node_index[gk], node_index[tk])\n",
    "\n",
    "        total_pairs += len(gi)\n",
    "        print(f\"Unioned {total_pairs:,} pairs, nodes {len(node_index):,}\")\n",
    "\n",
    "    # compress to roots and compact to entity_id, 0..E-1\n",
    "    roots = {}\n",
    "    root_to_entity = {}\n",
    "    entity_ids = np.empty(len(node_index), dtype=np.int64)\n",
    "    for key, idx in node_index.items():\n",
    "        r = dsu.find(idx)\n",
    "        roots[idx] = r\n",
    "        if r not in root_to_entity:\n",
    "            root_to_entity[r] = len(root_to_entity)\n",
    "        entity_ids[idx] = root_to_entity[r]\n",
    "\n",
    "    print(f\"Built {len(root_to_entity):,} entities across {len(node_index):,} nodes\")\n",
    "\n",
    "    return node_index, entity_ids, root_to_entity\n",
    "\n",
    "# ---------- Write enriched outputs in streaming mode ----------\n",
    "def write_with_entity(geo_in: str, tx_in: str, node_index: dict, entity_ids: np.ndarray,\n",
    "                      geo_out: str, tx_out: str, chunk: int = CHUNK):\n",
    "    # make reverse map index -> entity_id for fast lookups\n",
    "    idx_to_entity = entity_ids  # alias, same array\n",
    "    # lazily create outputs\n",
    "    for p in [geo_out, tx_out]:\n",
    "        try:\n",
    "            Path(p).unlink()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    link_rows = 0\n",
    "    for gi, ti in zip(\n",
    "        pd.read_csv(geo_in, chunksize=chunk, low_memory=False),\n",
    "        pd.read_csv(tx_in,  chunksize=chunk, low_memory=False),\n",
    "    ):\n",
    "        if not np.array_equal(gi[\"link_id\"].values, ti[\"link_id\"].values):\n",
    "            raise RuntimeError(\"link_id order mismatch during write phase\")\n",
    "\n",
    "        # look up entity for each row using the corresponding ids\n",
    "        g_keys = (\"g:\" + gi[\"client_id\"].astype(str)).values\n",
    "        t_keys = (\"t:\" + ti[\"nameOrig\"].astype(str)).values\n",
    "\n",
    "        # both sides should map to the same entity, take either\n",
    "        ent = np.fromiter((idx_to_entity[node_index[k]] for k in g_keys), dtype=np.int64, count=len(g_keys))\n",
    "        gi_out = gi.copy()\n",
    "        ti_out = ti.copy()\n",
    "        gi_out.insert(1, \"entity_id\", ent)\n",
    "        ti_out.insert(1, \"entity_id\", ent)\n",
    "\n",
    "        # append, preserve headers only on first chunk\n",
    "        mode = \"a\"\n",
    "        header = not Path(geo_out).exists()\n",
    "        gi_out.to_csv(geo_out, index=False, compression=\"gzip\", mode=\"w\" if header else mode, header=header)\n",
    "        ti_out.to_csv(tx_out,  index=False, compression=\"gzip\", mode=\"w\" if header else mode, header=header)\n",
    "        link_rows += len(gi)\n",
    "        print(f\"Wrote {link_rows:,} rows with entity_id\")\n",
    "\n",
    "# ---------- Small summary for QA ----------\n",
    "def write_entity_summary(geo_with_ent: str, tx_with_ent: str, out_path: str):\n",
    "    # count events and fraud per entity without full load\n",
    "    ent_counts = defaultdict(lambda: {\"geo\":0, \"tx\":0, \"fraud\":0})\n",
    "    for chunk in pd.read_csv(geo_with_ent, usecols=[\"entity_id\"], chunksize=CHUNK):\n",
    "        for e, c in chunk[\"entity_id\"].value_counts().items():\n",
    "            ent_counts[int(e)][\"geo\"] += int(c)\n",
    "    for chunk in pd.read_csv(tx_with_ent, usecols=[\"entity_id\",\"label_isFraud\"], chunksize=CHUNK):\n",
    "        vc = chunk.groupby(\"entity_id\")[\"label_isFraud\"].agg([\"count\",\"sum\"])\n",
    "        for e, row in vc.iterrows():\n",
    "            ent_counts[int(e)][\"tx\"]    += int(row[\"count\"])\n",
    "            ent_counts[int(e)][\"fraud\"] += int(row[\"sum\"])\n",
    "\n",
    "    rows = []\n",
    "    for e, d in ent_counts.items():\n",
    "        rows.append({\"entity_id\": e, \"geo_events\": d[\"geo\"], \"tx_events\": d[\"tx\"],\n",
    "                     \"tx_fraud\": d[\"fraud\"], \"tx_fraud_rate\": d[\"fraud\"] / d[\"tx\"] if d[\"tx\"] else 0.0})\n",
    "    df = pd.DataFrame(rows).sort_values(\"tx_events\", ascending=False)\n",
    "    df.to_csv(out_path, index=False, compression=\"gzip\")\n",
    "    print(f\"Wrote {out_path}, entities {len(df)}\")\n",
    "\n",
    "def main():\n",
    "    Path(BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1, build the entity map by streaming pairs\n",
    "    node_index, entity_ids, root_to_entity = build_entity_map(GEO_IN, TX_IN, chunk=CHUNK)\n",
    "\n",
    "    # 2, write enriched files with entity_id added\n",
    "    write_with_entity(GEO_IN, TX_IN, node_index, entity_ids, GEO_OUT, TX_OUT, chunk=CHUNK)\n",
    "\n",
    "    # 3, write a small entity summary for QA\n",
    "    write_entity_summary(GEO_OUT, TX_OUT, ENT_SUM)\n",
    "\n",
    "    print(\"\\nDone\")\n",
    "    print(f\"Geo with entity, {GEO_OUT}\")\n",
    "    print(f\"Tx  with entity, {TX_OUT}\")\n",
    "    print(f\"Entity summary, {ENT_SUM}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe4b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
