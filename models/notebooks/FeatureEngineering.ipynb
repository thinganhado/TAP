{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0a5ed9-0533-4e94-a02e-2b224103d4e1",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 1: Load Processed Data</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78691d5b-1094-4503-9a6a-2fbb3914dec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Loaded 66,295,724 events\n",
      "Section 1 completed: Data loaded successfully\n",
      "Data shape: (66295724, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_processed_data(file_path=\"geo_data_fully_processed.pkl\"):\n",
    "    \"\"\"Load the processed data from Step 3\"\"\"\n",
    "    print(\"Loading processed data...\")\n",
    "    geo_df = pd.read_pickle(file_path)\n",
    "    print(f\"Loaded {len(geo_df):,} events\")\n",
    "    return geo_df\n",
    "\n",
    "# Execute Section 1\n",
    "df = load_processed_data()\n",
    "print(\"Section 1 completed: Data loaded successfully\")\n",
    "print(f\"Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ba37b-c7e3-4153-b809-9533e00a8a36",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 1b: Extract 10% Sample of the Dataset</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc8f87c-dd36-4502-96ae-8acec5605e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 10.0% sample using client_based method...\n",
      "Original dataset: 66,295,724 rows\n",
      "Sample dataset: 6,745,765 rows\n",
      "Sample represents 10.18% of original data\n",
      "Original clients: 72,573\n",
      "Sample clients: 7,257\n",
      "Section 1b completed: 10% sample extracted\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_data_sample(df, sample_fraction=0.1, random_state=42, method='random'):\n",
    "    \"\"\"\n",
    "    Extract a sample from the dataset for initial testing\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame to sample from\n",
    "    sample_fraction: Fraction of data to sample (0.0 to 1.0)\n",
    "    random_state: Random seed for reproducibility\n",
    "    method: Sampling method ('random', 'client_based', or 'time_based')\n",
    "    \"\"\"\n",
    "    print(f\"Extracting {sample_fraction*100}% sample using {method} method...\")\n",
    "    \n",
    "    if method == 'random':\n",
    "        # Simple random sampling\n",
    "        sample_df = df.sample(frac=sample_fraction, random_state=random_state)\n",
    "        \n",
    "    elif method == 'client_based':\n",
    "        # Sample by clients to maintain client behavior patterns\n",
    "        unique_clients = df['client_id'].unique()\n",
    "        sample_clients = np.random.choice(unique_clients, \n",
    "                                         size=int(len(unique_clients) * sample_fraction), \n",
    "                                         replace=False)\n",
    "        sample_df = df[df['client_id'].isin(sample_clients)]\n",
    "        \n",
    "    elif method == 'time_based':\n",
    "        # Sample by time period to maintain temporal patterns\n",
    "        time_range = df['event_time'].max() - df['event_time'].min()\n",
    "        sample_start = df['event_time'].min() + time_range * np.random.random()\n",
    "        sample_end = sample_start + time_range * sample_fraction\n",
    "        sample_df = df[(df['event_time'] >= sample_start) & (df['event_time'] <= sample_end)]\n",
    "    \n",
    "    print(f\"Original dataset: {len(df):,} rows\")\n",
    "    print(f\"Sample dataset: {len(sample_df):,} rows\")\n",
    "    print(f\"Sample represents {len(sample_df)/len(df)*100:.2f}% of original data\")\n",
    "    \n",
    "    # Verify we have a reasonable number of clients in the sample\n",
    "    if 'client_id' in df.columns:\n",
    "        original_clients = df['client_id'].nunique()\n",
    "        sample_clients = sample_df['client_id'].nunique()\n",
    "        print(f\"Original clients: {original_clients:,}\")\n",
    "        print(f\"Sample clients: {sample_clients:,}\")\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Execute Section 1b: Extract 10% sample\n",
    "# Choose one of the sampling methods:\n",
    "# method='random' - Simple random sampling (fastest)\n",
    "# method='client_based' - Sample by clients (preserves client behavior patterns)\n",
    "# method='time_based' - Sample by time period (preserves temporal patterns)\n",
    "\n",
    "sample_df = extract_data_sample(df, sample_fraction=0.1, random_state=42, method='client_based')\n",
    "\n",
    "# Replace the original dataframe with the sample for further processing\n",
    "df = sample_df.copy()\n",
    "\n",
    "print(\"Section 1b completed: 10% sample extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba76932-923a-400c-9d30-b03c83dceeff",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 2: Basic Movement Features</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1e9866-5f6a-492e-8c3e-a39aec42010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating basic movement features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distances: 100%|██████████| 6745765/6745765 [17:37<00:00, 6377.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 2 completed: Basic movement features calculated\n",
      "                                                  client_id  \\\n",
      "66121636  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "66121637  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "66121638  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "66121639  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "66121640  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "\n",
      "                  event_time  distance_km  velocity_kmh  \n",
      "66121636 2022-01-08 08:23:53          0.0           NaN  \n",
      "66121637 2022-01-09 06:20:43          0.0           0.0  \n",
      "66121638 2022-01-10 06:17:25          0.0           0.0  \n",
      "66121639 2022-01-11 14:31:30          0.0           0.0  \n",
      "66121640 2022-01-14 07:57:30          0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "# Calculate basic movement features like velocity and distance between points\n",
    "def calculate_basic_movement_features(df):\n",
    "    print(\"Calculating basic movement features...\")\n",
    "    \n",
    "    # Sort by client and time\n",
    "    df = df.sort_values(['client_id', 'event_time']).copy()\n",
    "    \n",
    "    # Calculate time differences\n",
    "    df['time_diff_hours'] = df.groupby('client_id')['event_time'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate distance between consecutive points\n",
    "    df['prev_lat'] = df.groupby('client_id')['latitude'].shift()\n",
    "    df['prev_lon'] = df.groupby('client_id')['longitude'].shift()\n",
    "    \n",
    "    # Calculate distance using geodesic\n",
    "    tqdm.pandas(desc=\"Calculating distances\")\n",
    "    df['distance_km'] = df.progress_apply(\n",
    "        lambda row: geodesic((row['prev_lat'], row['prev_lon']), \n",
    "                            (row['latitude'], row['longitude'])).km \n",
    "        if not pd.isna(row['prev_lat']) else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate velocity (km/h)\n",
    "    df['velocity_kmh'] = df['distance_km'] / df['time_diff_hours']\n",
    "    df['velocity_kmh'] = df['velocity_kmh'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    df = df.drop(['prev_lat', 'prev_lon'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute Section 2\n",
    "df = calculate_basic_movement_features(df)\n",
    "print(\"Section 2 completed: Basic movement features calculated\")\n",
    "print(df[['client_id', 'event_time', 'distance_km', 'velocity_kmh']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eae725-09ef-4cf3-b063-4b6068527359",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 3: Suspicious Location Detection</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4a4628-6bb4-4853-9d66-abf6262fa8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 3 completed: Suspicious location helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def is_in_ocean(lat, lon):\n",
    "    \"\"\"Check if coordinates are in major ocean areas\"\"\"\n",
    "    # Pacific Ocean (approximate bounding box)\n",
    "    if (-60 <= lat <= 60) and (-180 <= lon <= -70) or (130 <= lon <= 180):\n",
    "        return True\n",
    "    \n",
    "    # Atlantic Ocean\n",
    "    if (-60 <= lat <= 60) and (-70 <= lon <= 20):\n",
    "        return True\n",
    "    \n",
    "    # Indian Ocean\n",
    "    if (-60 <= lat <= 30) and (20 <= lon <= 120):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_in_polar_region(lat, lon):\n",
    "    \"\"\"Check if coordinates are in polar regions\"\"\"\n",
    "    # Antarctica\n",
    "    if lat < -60:\n",
    "        return True\n",
    "    \n",
    "    # Remote Arctic\n",
    "    if lat > 75 and abs(lon) > 150:  # Very remote arctic areas\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def is_in_desert(lat, lon):\n",
    "    \"\"\"Check if coordinates are in major desert areas\"\"\"\n",
    "    # Sahara Desert\n",
    "    if (15 <= lat <= 30) and (-15 <= lon <= 40):\n",
    "        return True\n",
    "    \n",
    "    # Arabian Desert\n",
    "    if (20 <= lat <= 30) and (35 <= lon <= 60):\n",
    "        return True\n",
    "    \n",
    "    # Gobi Desert\n",
    "    if (40 <= lat <= 45) and (90 <= lon <= 115):\n",
    "        return True\n",
    "    \n",
    "    # Australian Outback\n",
    "    if (-30 <= lat <= -20) and (120 <= lon <= 140):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_in_international_waters(lat, lon):\n",
    "    \"\"\"\n",
    "    Check if coordinates are far from any coastline\n",
    "    Simplified version - assumes ocean coordinates far from land are suspicious\n",
    "    \"\"\"\n",
    "    if is_in_ocean(lat, lon):\n",
    "        # If it's in ocean and far from the edges of continents, consider it international waters\n",
    "        if (abs(lon) > 100 and abs(lat) < 30) or (abs(lon) < 20 and lat < -40):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"Section 3 completed: Suspicious location helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe670256-729d-4e93-85c9-45069183952e",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 4: Add Suspicious Location Features</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536d57d0-1faf-46fe-9bc7-ece78aea3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding suspicious location features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking ocean locations: 100%|██████████| 6745765/6745765 [01:10<00:00, 96121.94it/s] \n",
      "Checking polar regions: 100%|██████████| 6745765/6745765 [01:09<00:00, 96523.67it/s] \n",
      "Checking desert areas: 100%|██████████| 6745765/6745765 [01:09<00:00, 97159.65it/s] \n",
      "Checking international waters: 100%|██████████| 6745765/6745765 [01:10<00:00, 95244.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 4 completed: Suspicious location features added\n",
      "                                                  client_id  is_ocean  \\\n",
      "66121636  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     False   \n",
      "66121637  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     False   \n",
      "66121638  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     False   \n",
      "66121639  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     False   \n",
      "66121640  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     False   \n",
      "\n",
      "          is_polar  is_desert  is_international_waters  \\\n",
      "66121636      True      False                    False   \n",
      "66121637      True      False                    False   \n",
      "66121638      True      False                    False   \n",
      "66121639      True      False                    False   \n",
      "66121640      True      False                    False   \n",
      "\n",
      "          suspicious_location_score  \n",
      "66121636                        0.3  \n",
      "66121637                        0.3  \n",
      "66121638                        0.3  \n",
      "66121639                        0.3  \n",
      "66121640                        0.3  \n"
     ]
    }
   ],
   "source": [
    "# SECTION 4: Add Suspicious Location Features\n",
    "def add_suspicious_location_features(df):\n",
    "    print(\"Adding suspicious location features...\")\n",
    "    \n",
    "    tqdm.pandas(desc=\"Checking ocean locations\")\n",
    "    df['is_ocean'] = df.progress_apply(lambda row: is_in_ocean(row['latitude'], row['longitude']), axis=1)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Checking polar regions\")\n",
    "    df['is_polar'] = df.progress_apply(lambda row: is_in_polar_region(row['latitude'], row['longitude']), axis=1)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Checking desert areas\")\n",
    "    df['is_desert'] = df.progress_apply(lambda row: is_in_desert(row['latitude'], row['longitude']), axis=1)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Checking international waters\")\n",
    "    df['is_international_waters'] = df.progress_apply(\n",
    "        lambda row: is_in_international_waters(row['latitude'], row['longitude']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Composite suspicious location score\n",
    "    df['suspicious_location_score'] = (\n",
    "        df['is_ocean'].astype(int) * 0.4 +\n",
    "        df['is_polar'].astype(int) * 0.3 +\n",
    "        df['is_desert'].astype(int) * 0.2 +\n",
    "        df['is_international_waters'].astype(int) * 0.1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute Section 4\n",
    "df = add_suspicious_location_features(df)\n",
    "print(\"Section 4 completed: Suspicious location features added\")\n",
    "print(df[['client_id', 'is_ocean', 'is_polar', 'is_desert', 'is_international_waters', 'suspicious_location_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e0d04-57bb-42e4-a35a-9f6be27cd8f6",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 5: Calculate home location (most common coordinates)</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16aee98-0ebf-4ea6-8b37-21f43b1256ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating client behavioral baselines...\n",
      "Section 5 completed: Client behavioral baselines calculated\n",
      "                                           client_id  home_latitude  \\\n",
      "0  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "1  0002ddd816198d32474486d54f4bfe4f7b361119b5dc45...     -33.486328   \n",
      "2  0008482a86bca0ad595a949f9e314e157a288c332db269...     -89.978027   \n",
      "3  000f4309610cc90124943138fdf6d50a2b5967a9ba79b2...     -75.322266   \n",
      "4  000fff599dacf53fffcf4663ab370d21921d1988f8cfad...     -89.978027   \n",
      "\n",
      "   home_longitude  avg_velocity_kmh  velocity_std_kmh  \n",
      "0     -134.934082        403.656888       3731.853505  \n",
      "1     -118.652344        992.204393       3945.654443  \n",
      "2     -134.934082      14731.870949     266677.079131  \n",
      "3     -122.167969          0.095655          0.165679  \n",
      "4     -134.934082        179.631911       1584.836514  \n"
     ]
    }
   ],
   "source": [
    "def calculate_client_baselines(df):\n",
    "    \"\"\"Calculate client-specific behavioral baselines\"\"\"\n",
    "    print(\"Calculating client behavioral baselines...\")\n",
    "    \n",
    "    # Group by client\n",
    "    client_groups = df.groupby('client_id')\n",
    "    \n",
    "    # Calculate home location (most common coordinates)\n",
    "    home_locations = client_groups.agg({\n",
    "        'latitude': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "        'longitude': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "    home_locations.columns = ['client_id', 'home_latitude', 'home_longitude']\n",
    "    \n",
    "    # Calculate average velocity per client\n",
    "    avg_velocity = client_groups['velocity_kmh'].mean().reset_index()\n",
    "    avg_velocity.columns = ['client_id', 'avg_velocity_kmh']\n",
    "    \n",
    "    # Calculate velocity standard deviation per client\n",
    "    velocity_std = client_groups['velocity_kmh'].std().reset_index()\n",
    "    velocity_std.columns = ['client_id', 'velocity_std_kmh']\n",
    "    \n",
    "    # Merge all client baseline features\n",
    "    client_baselines = home_locations.merge(avg_velocity, on='client_id').merge(velocity_std, on='client_id')\n",
    "    \n",
    "    return client_baselines\n",
    "\n",
    "# Execute Section 5\n",
    "client_baselines = calculate_client_baselines(df)\n",
    "print(\"Section 5 completed: Client behavioral baselines calculated\")\n",
    "print(client_baselines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125e94-e45a-4ace-875d-9cabb205bd0e",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 6: Calculate distance from home</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc968e2-7b08-4c83-ae99-7252658927a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding client baseline features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating distance from home: 100%|██████████| 6745765/6745765 [16:09<00:00, 6960.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 6 completed: Client baseline features added\n",
      "                                           client_id  home_latitude  \\\n",
      "0  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "1  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "2  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "3  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "4  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...     -89.978027   \n",
      "\n",
      "   home_longitude  distance_from_home_km  velocity_z_score  \n",
      "0     -134.934082                    0.0               NaN  \n",
      "1     -134.934082                    0.0         -0.108165  \n",
      "2     -134.934082                    0.0         -0.108165  \n",
      "3     -134.934082                    0.0         -0.108165  \n",
      "4     -134.934082                    0.0         -0.108165  \n"
     ]
    }
   ],
   "source": [
    "def add_client_baseline_features(df, client_baselines):\n",
    "    \"\"\"Add client baseline features to the main dataframe\"\"\"\n",
    "    print(\"Adding client baseline features...\")\n",
    "    \n",
    "    # Merge client baselines\n",
    "    df = df.merge(client_baselines, on='client_id', how='left')\n",
    "    \n",
    "    # Calculate distance from home\n",
    "    tqdm.pandas(desc=\"Calculating distance from home\")\n",
    "    df['distance_from_home_km'] = df.progress_apply(\n",
    "        lambda row: geodesic((row['home_latitude'], row['home_longitude']), \n",
    "                            (row['latitude'], row['longitude'])).km, axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate velocity anomalies (z-score)\n",
    "    df['velocity_z_score'] = (df['velocity_kmh'] - df['avg_velocity_kmh']) / df['velocity_std_kmh']\n",
    "    df['velocity_z_score'] = df['velocity_z_score'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute Section 6\n",
    "df = add_client_baseline_features(df, client_baselines)\n",
    "print(\"Section 6 completed: Client baseline features added\")\n",
    "print(df[['client_id', 'home_latitude', 'home_longitude', 'distance_from_home_km', 'velocity_z_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfdd22a-6f63-416a-8721-e42e6740e9e4",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 7: Time-based Features</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414b4e0a-bbea-4bac-a200-c41c7a841ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding temporal features...\n",
      "Section 7 completed: Temporal features added\n",
      "                                           client_id          event_time  \\\n",
      "0  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51... 2022-01-08 08:23:53   \n",
      "1  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51... 2022-01-09 06:20:43   \n",
      "2  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51... 2022-01-10 06:17:25   \n",
      "3  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51... 2022-01-11 14:31:30   \n",
      "4  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51... 2022-01-14 07:57:30   \n",
      "\n",
      "   hour_of_day  is_night  is_weekend  \n",
      "0            8         0           1  \n",
      "1            6         1           1  \n",
      "2            6         1           0  \n",
      "3           14         0           0  \n",
      "4            7         0           0  \n"
     ]
    }
   ],
   "source": [
    "def add_temporal_features(df):\n",
    "    \"\"\"Add time-based features\"\"\"\n",
    "    print(\"Adding temporal features...\")\n",
    "    \n",
    "    # Time of day features\n",
    "    df['hour_of_day'] = df['event_time'].dt.hour\n",
    "    df['is_night'] = ((df['hour_of_day'] >= 22) | (df['hour_of_day'] <= 6)).astype(int)\n",
    "    df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "    \n",
    "    # Day of week features\n",
    "    df['day_of_week'] = df['event_time'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Month feature\n",
    "    df['month'] = df['event_time'].dt.month\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute Section 7\n",
    "df = add_temporal_features(df)\n",
    "print(\"Section 7 completed: Temporal features added\")\n",
    "print(df[['client_id', 'event_time', 'hour_of_day', 'is_night', 'is_weekend']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef25fc-a859-4581-87bd-257cdb4b644e",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 8: Composite Suspicion Score</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d443de-e125-4c3b-b20b-d5d8ba9c02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating composite suspicion score...\n",
      "Section 8 completed: Composite suspicion score calculated\n",
      "                                           client_id  \\\n",
      "0  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "1  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "2  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "3  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "4  0001edbc5ab720f70a615ed9e8429df9b6c3f3c3999a51...   \n",
      "\n",
      "   suspicious_location_score  distance_score  velocity_anomaly_score  \\\n",
      "0                        0.3             0.0                     NaN   \n",
      "1                        0.3             0.0                0.036055   \n",
      "2                        0.3             0.0                0.036055   \n",
      "3                        0.3             0.0                0.036055   \n",
      "4                        0.3             0.0                0.036055   \n",
      "\n",
      "   time_anomaly_score  composite_suspicion_score  \n",
      "0                 0.4                        NaN  \n",
      "1                 1.0                   0.299014  \n",
      "2                 0.6                   0.219014  \n",
      "3                 0.0                   0.099014  \n",
      "4                 0.0                   0.099014  \n"
     ]
    }
   ],
   "source": [
    "def calculate_composite_suspicion_score(df):\n",
    "    \"\"\"Calculate a composite suspicion score combining all features\"\"\"\n",
    "    print(\"Calculating composite suspicion score...\")\n",
    "    \n",
    "    # Normalize features for scoring\n",
    "    max_distance = df['distance_from_home_km'].max()\n",
    "    df['distance_score'] = df['distance_from_home_km'] / max_distance if max_distance > 0 else 0\n",
    "    \n",
    "    # Velocity anomaly score (absolute z-score, capped at 3)\n",
    "    df['velocity_anomaly_score'] = np.abs(df['velocity_z_score']).clip(0, 3) / 3\n",
    "    \n",
    "    # Time anomaly score (higher weight for night and weekend)\n",
    "    df['time_anomaly_score'] = (df['is_night'] * 0.6 + df['is_weekend'] * 0.4)\n",
    "    \n",
    "    # Composite score (weights can be adjusted)\n",
    "    df['composite_suspicion_score'] = (\n",
    "        df['suspicious_location_score'] * 0.3 +\n",
    "        df['distance_score'] * 0.25 +\n",
    "        df['velocity_anomaly_score'] * 0.25 +\n",
    "        df['time_anomaly_score'] * 0.2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute Section 8\n",
    "df = calculate_composite_suspicion_score(df)\n",
    "print(\"Section 8 completed: Composite suspicion score calculated\")\n",
    "print(df[['client_id', 'suspicious_location_score', 'distance_score', \n",
    "          'velocity_anomaly_score', 'time_anomaly_score', 'composite_suspicion_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157a3ff-ce40-4d66-b337-a1baa79a863e",
   "metadata": {},
   "source": [
    "<h3><strong>SECTION 9: Save Results</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "937dbfb2-eb2e-474b-8a06-3b16b5b3354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving engineered features...\n",
      "Features saved to geo_data_with_features.pkl\n",
      "Section 9 completed: Engineered features saved\n",
      "All feature engineering steps completed successfully!\n",
      "Final dataset shape: (6745765, 32)\n"
     ]
    }
   ],
   "source": [
    "def save_engineered_features(df, output_file=\"geo_data_with_features.pkl\"):\n",
    "    \"\"\"Save the dataframe with all engineered features\"\"\"\n",
    "    print(\"Saving engineered features...\")\n",
    "    df.to_pickle(output_file)\n",
    "    print(f\"Features saved to {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Execute Section 9\n",
    "df = save_engineered_features(df)\n",
    "print(\"Section 9 completed: Engineered features saved\")\n",
    "print(\"All feature engineering steps completed successfully!\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc17d8-efc4-493f-a263-e4cd60de50c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
